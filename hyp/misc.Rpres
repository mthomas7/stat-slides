Miscellaneous Tests
========================================================
author: Math 145
autosize: true
width:1920

Proportion Test
===


```{r include=FALSE}
ames <- read.csv("~/Dropbox/teaching/slides/stat-slides/hyp/ames.csv")
class <- read.csv("~/Dropbox/teaching/slides/stat-slides/hyp/Fall_2015_Class_Data_Cleaned.csv")
library(mosaic)
```

Analogous to t-test, but for proportions
```{r}
tally(~Lot.Shape, data=ames)
```

***

```{r}
prop.test(~Lot.Shape, p=.25, data=ames)
```

Chi-square test: Multiple categorical variables
===

ABC Poll of 1500 adults, 750 men, 750 women on Presidential approval rating

On March 10, 2002, 623 men, 607 women approved of Bush’s handling of job. Was there a gender gap in the approval ratings significant at the 5% level?

On Sept 9, 2001, 443 men out of 750 and 390 women out of 750 approved of Bush’s handling of
job. Is there a gender gap in the approval ratings significant at the 5% level?

Looking at counts
===
Create two-way tables for the data in the previous two problems

Create *joint* and *conditional* probability tables

Which conditional probability table is more interesting to us right now?

An Expected Count Table
===
* What would you expect if there were no relationship between the two variables? Use the same total count in each grouping.

* $Expected \ cell \ count = \dfrac{Row \ total \times Column \ total}{n}$

Difference tables
===
* Now we make a table of differences, with Observed - Expected
* What differences do you see between the tables of differences that may indicate significance?
* What do you notice about the sums of the differences in one table? How many values are needed to
determine all the others?
* *We say the table has one degree of freedom.*

INFERENCE FOR TWO-WAY TABLES: CHI SQUARE TEST FOR INDEPENDENCE
===

Null hypothesis: There is no association between variables.

*This means the columns have same % breakdown. In addition, the rows have same % break down.*

Alternative hypothesis: There is an association between the variables, but we do not specify what.

Write the null and alternative hypotheses for the example we're working on.

Chi-Square Test Statistic
===
$$\chi^2 = \sum \dfrac{(observed \ count - expected \ count)^2}{expected \ count}$$

with df = (# rows - 1)(# columns - 1)

What would a large or small value of $\chi^2$ mean?

Perform the Chi-Square hypothesis test for 2002, 2001
===

* Step 1: Hypotheses
* Step 2: Test statistic
* Step 3: p-value
* Step 4: Conclusion

* Interpret the p-value

Notes on Using the Chi-Square Test
===

* Rejecting null hypothesis does not tell us which way the interaction happens.
* Since $\chi^2$ is always positive, there is only an upper tail. We never multiply the probabilities by 2.
* Conditions: We can use the Chi-Square test when:
  * For tables larger than $2 \times 2$: Average of expected counts $\geq 5$; all expected counts $\geq 1$
  * For $2 \times 2$ tables: All expected counts $\geq 5$
  
Is the following drug effective?
===

In an experiment, 600 people were divided randomly into three groups and given a placebo, a single dose, or a double dose of a drug. The observed results were:

\      | Placebo | Single | Double |         
-------|---------|--------|--------|-------
Improve| 50      | 57     | 78     | 
Didn't | 180     | 133    | 102    |

Result
===

$\chi^2 = 22.2$

What can we say?

Interpreting
===

Since the Chi-Squared test does not tell us how the interaction happens if we reject the null
hypothesis, how do we know if the drug is effective? (Maybe it makes people worse?)

Create the conditional distribution table. Which way should we make it?

Further analysis
===

Is a double dose of the drug significantly better than single dose?

$\chi^2=7.09$

Make a conditional probability table to interpret!

Working and childcare
===

As more women work, the effect of childcare on infants has been investigated. One study
looked at the relationship between infant-mother attachment and the time spent in childcare, with the
following results:

\        | Low                   | Moderate           | High             
---------|-----------------------|--------------------|------------------
         | (0-3 hours/week)      | (4-19 hours/week)  | (20-54 hours/week)
Anxious  | 24                    | 35                 | 5
Secure   | 11                    | 10                 | 8

<small>By J. Jacobson and D. Willie, reported in Statistical Record of Women Worldwide (Galen Research, 1991) and Introduction to Practice of Statistics, 13-th ed, Mendenhall, Beaver and Beaver.</small>

Result
===

Does the data provide evidence of a difference in attachment patterns with the amount of time spent
in childcare? Give the hypotheses, the test statistic, and the p-value with your conclusion. Use a 1% confidence level. What would change if we had used a 5% confidence level?

$\chi^2 = 7.27$

Further analysis
===

Combine the moderate and high childcare groups together, and test again. What is your conclusion?

$\chi^2 = 0.00158$

In R: chisq.test()
===

```{r}
tally(Street~Land.Contour, data=ames)
```
***
```{r}
chisq.test(tally(Street~Land.Contour, data=ames))
```

Regression
===
type:section

Scatterplots
===

How do we analyze a pair of variables which are both numerical/continuous?

How could we investigate the relationship between apps on phone and ounces of water drunk?

***

```{r}
plotPoints(Number.of.apps.on.phone~Number.of.ounces.of.water.drunk.per.day, data=class)
```


Best fit line
===

How could we determine a best fit line for the data?

***

```{r}
plotPoints(Number.of.apps.on.phone~Number.of.ounces.of.water.drunk.per.day, data=class)
```

The best fit line
===

```{r eval=FALSE}
model1 <- lm(Number.of.apps.on.phone~Number.of.ounces.of.water.drunk.per.day, data=class)
{plot(Number.of.apps.on.phone~Number.of.ounces.of.water.drunk.per.day, data=class)
abline(model1)}
```

***

```{r echo=FALSE}
model1 <- lm(Number.of.apps.on.phone~Number.of.ounces.of.water.drunk.per.day, data=class)
{plot(Number.of.apps.on.phone~Number.of.ounces.of.water.drunk.per.day, data=class)
abline(model1)}
```

Description of best fit line
===
```{r}
model1 <- lm(Number.of.apps.on.phone~Number.of.ounces.of.water.drunk.per.day, data=class)
summary(model1)
```


Why find a best fit line
===

Extrapolation vs interpolation

<center>
<img src="extrapolate.png" height=500>
</center>

Correlation
===

To measure the strength of an association, we use the correlation coefficient, $r$.

$$r = \dfrac{1}{n-1}\displaystyle\sum_{i=1}^{i=n}\dfrac{x_i-\bar{x}}{s_x} \dfrac{y_i-\bar{y}}{s_y}$$

* What values can $r$ be?
* What does this value tell us?
* $r^2$ is also good for interpretation

Examples
===

$r^2$?

<center>
![](1.png)
</center>

***

<center>
![](2.png)
</center>

===

$r^2$?

<center>
![](3.png)
</center>


The correlation coefficient (r)
===

* What does the correlation coefficient *mean*?
* What if we have a flat line?
* Parabolic?
* Strong vs weak? Linear vs non-linear?


r Squared
===

<center>
<img src="r2.png" height=500>
</center>

Regression
===

<center>
![](loveregression.gif)
</center>

Residuals
===

Residuals are the errors for each term

What do these values look like graphically?

Conditions for least squares line
===

* Linearity
* Nearly normal residuals
* Constant variability
* Independent observations

What does the following abstract tell us?
===

The present study is based on the association of hand grip strength (both left and right) with height, weight and BMI
on randomly selected 600 normal healthy individuals(300 boys and 300 girls) aged 6-25 years of Amritsar, Punjab.
The findings of present study indicate a strong association of right and left hand grip strength with height ($r=0.925$ and $r=0.927$ respectively in boys and $r=0.800$ and $r=0.786$ respectively in girls), weight ($r=0.882$ and $r=0.878$ respectively in boys and $r=0.698$ and $r=0.690$ respectively in girls) and with BMI ($r=0.636$ and $r=0.632$ respectively in boys and $r=0.477$ and $r=0.472$ respectively in girls).

<small>From “An Association of Hand Grip Strength with Height, Weight and BMI in Boys and Girls aged 6-25 years of Amritsar, Punjab, India” Koley, Gandhi, Singh The Internet Journal of Biological Anthropology . 2008. Volume 2 Number 1.</small>

Causation
===

What can we say about causation in the previous example?

Causation
===
<center>
<img src="cell_phones.png" height=500>
</center>

===

What is the trend in the age of marriage in the US?

<center>
<img src="4.png" height=500>
</center>

===

Using the data from 1960-2005, we fit a line and use it to predict the age of marriage for men and women in 2010. In what year is the marriage age for men predicted to reach 30? Comment on the reliability of these answers.

<center>
<img src="5.png" width=1000>
</center>

* What do each of the numbers mean? What are the units?
* If the current trends continue, will men’s and women’s marriage ages ever be equal? If so, when?

Leaning tower of Pisa
===

![](6.png)

***

The Tower of Pisa is leaning more each year. The measurements show the lean in tenths of
millimeter beyond 2.9 meters. Thus in 1975, the Tower was leaning 2.9642 meter from the vertical.

1. Does it look like the lean of the Tower is increasing with time? If so, how fast is the lean increasing with time?
2. Is there evidence that the lean changes significantly with time?



Regression line
===

Use the scatterplot and regression equation to describe how the lean changing with time.
Interpret the slope and vertical intercept.
![](7.png)

How can we decide if the lean is increasing significantly with time?
===

Do a hypothesis test. The regression equation was derived from a sample. It is a sample regression line.

The slope and intercepts in this sample regression equation are estimates, based on the particular sample we had. If we took another sample (for example, if we measured the tower at different times of year), we would probably get a different slope and a different intercept. They are *sample statistics*.

What we are really interested are the population parameters—that is, the slope and intercept of the line
that first the whole population, not just these sample values. This is the *population regression line*.

Questions
===

Is it possible that the population regression equation has

* A different slope than the one we found, 9.32?
* A slope of different sign?
* A slope of 0?
    * It is certainly likely we’d get a different slope. The 9.32 is an estimate, and if we took a different sample, we’d expect to get a different slope.
    * Whether or not we’d get negative slope depends on whether the lean really is increasing.
* We’d like to know if the slope is significant different that 0.
    * We do a hypothesis test on the sample slope to find out.

Hypothesis Test that the Slope of Population Regression Line is Significantly Different than 0
===

Null hypothesis: Slope of regression line is 0. This means that there is no relation between lean and year.

Alternate hypothesis: Slope of regression line is different from 0. There means there is a relationship
between lean and year.

$\alpha$ level = 0.05

Test statistic: Turns out to be a t-statistic, with $n-2$ degrees of freedom, where $n$ is the number of data
points.

$p$-value: ?

How are the t-statistics and P-values for the slope calculated?
===

$$t=\dfrac{sample-population}{standard\ error}$$

For the slope:

$$t = \dfrac{9.31868132-0}{standard \ error}$$

How do we compute the standard error?

$$SE = \sqrt{\dfrac{\sum \epsilon_i^2}{n-2}}$$ (where $\epsilon_i = y_i - \hat{y}_i$)

So we get
===

$$t = \dfrac{9.31868132-0}{0.3099142} = 30.06858$$

p-value is found using a computer to be $6.5 \times 10^{-12}$, which is very small, or use t-table

Conclusion
===

The probability of seeing, by chance, a slope as different from 0 as we did if there was no
relationship between the lean and time is $6.5 \times 10^{-12}$. Thus, we reject the null the idea that there is no
relationship between lean and year; we conclude there is a *linear* relationship.

(Though this may not be the *best* model)

Note
===

We will also be seeing a t-value, SE, and p-value for the intercept. What do you think this tells us, and why might we be interested in it?

Why no r's reported?
===

Why do we keep seeing $R^2$?

Important connections and notation
===

Sample regression line: $\hat{y} = b_0 + b_1 x$ (hat means estimated, just like proportions)

Population regression line: $\mu_y = \beta_0 + \beta_1 x$

$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$

$b_1 = r \dfrac{s_y}{s_x}$

$b_0 = \bar{y}-b_1 \bar{x}$

Which are population parameters and which are sample statistics?
